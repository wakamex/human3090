# QwQ-32B â€” reasoning model, re-run with fixed pipeline
model: /seagate/models/QwQ-32B-Q4_K_M.gguf
benchmarks: [lcb]
temperature: 0.6
max_tokens: 10000
context_size: 32768
gpu_layers: 80
top_p: 0.9
min_p: 0.1
problems_file: test5.jsonl
save_raw: true
