# Qwen3.5-35B-A3B â€” MoE thinking model (3B active), needs 32k context
model: /mnt/raid5/models/Qwen3.5-35B-A3B-Q4_K_M.gguf
benchmarks: [human_eval, lcb]
max_tokens: 32000
context_size: 32768
save_raw: true
